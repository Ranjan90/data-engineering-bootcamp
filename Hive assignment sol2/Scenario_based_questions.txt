Scenario Based questions:

Will the reducer work or not if you use “Limit 1” in any HiveQL query?
ans -- 
If we used only limit with select query then reducer will not work
Reducer mainly comes in picture in aggregate function.


Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time?
ans -- 
If by default metastore is configured with hive, then nultiple users can not access the hive at the same time. To make hive accessciable for multiple users at the same time, we need to configured hive with any embedded metastore


Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;
Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?
ans -- 
We can use partition on Month column, So we don't need to scan the complete table.
step 1- Create a partitioned table
create table part_by_month(cust_id INT, amount FLOAT, country STRING) partitioned by (month STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ;

Step 2- Enable dynaminc partitioning in hive.
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

step 3- transfer the data from non-partitioned table into partitioned table.
insert into table part_by_month partition(month) select cust_id,amount,country,month from transaction_details;


How can you add a new partition for the month December in the above partitioned table?
ans -- 
alter table part_by_month add patition (month = 'December') location '/part_by_month'


I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?
ans -- 
to fix this we need to enable dynamic partition.
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;


Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address
How will you consume this CSV file into the Hive warehouse using built-in SerDe?
ans --
we can use the SerDe in hive to work with .csv files. We can create a table and insert the data to store in warehouse.

create table sample(id int, first_name string, last_name string, email string, gender string, ip_address string)
row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'

load data inpath 'file:///temp/sample.csv' into table sample;


Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.
So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?
ans --
we can create external table

create external table all_file 
(id int, name string, e-mail string, country string) 
row format delimited 
fields terminated by ','
location '/input'


LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address;
The following statement failed to execute. What can be the cause?
ans -- inpath have dir only and filename is missing. this will raise an error


Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?
ans -- 
Yes we can add node in cluster
Step 1: Take a new system; create a new username and password
Step 2: Install SSH and with the master node setup SSH connections
Step 3: Add ssh public_rsa id key to the authorized keys file
Step 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file: 192.168.1.102 slave3.in slave3
Step 5: Start the DataNode on a new node
Step 6: Login to the new node like suhadoop or: ssh -X hadoop@192.168.1.103
Step 7: Start HDFS of the newly added slave node by using the following command: ./bin/hadoop-daemon.sh start data node
Step 8: Check the output of the jps command on the new node

